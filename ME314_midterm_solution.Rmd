---
title: "Midterm Assignemnt, ME314 2018"
output: html_document
---
 
<!-- ![LSE Logo](images/lse-logo.jpg) -->

#### Summer School 2018 midsession examination  

# ME314 Introduction to Data Science and Big Data 

## Suitable for all candidates          \

### Marking guide  

**Marking is done out of 100 points. All questions are weighted equally. 20 points each.**

### Instructions to candidates  

Time allowed: due 19:00 on Wednesday, 8th August 2018

Submit the assignment via Moodle

You will need to load the core library for the course textbook and libraries for LDA and KNN:
```{r}
library(ISLR)
library(MASS)
library(class)
```


This question should be answered using the `Weekly` data set, which is part of the `ISLR` package. This data contains 1,089 weekly stock returns for 21 years, from the beginning of 1990 to the end of 2010.

(1) Produce some numerical and graphical summaries of the `Weekly` data. Do there appear to be any patterns?

```{r}
summary(Weekly)
pairs(Weekly)
cor(Weekly[,-9])
```

**Year and Volume appear to have a relationship. No other patterns are discernible. This is a minimal answer and should get 10 points as it basically covers what's in the texbook lab session at the end of the chapter. Additional 10 points can be given for extra effort in exploratory data analysis. Using more developed visualisations of the relationships (if they identify patterns). We also encouraged them to use GGPLOT, so that can be reflected in the points.**

(2) Use the full data set to perform a logistic regression with `Direction` as the response and the five lag variables plus `Volume` as predictors. Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?

```{r}
attach(Weekly)
glm.fit <-  glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
              data = Weekly,
              family = binomial)
summary(glm.fit)
```

**Lag 2 is statistically significant at 0.05 level. That's the basic answer. The questions are very narrow and straightforward. So anything at the level of the basic answer gets full 20 points.**

(3) Compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.

```{r}
glm.probs <-  predict(glm.fit, type="response")
glm.pred <-  rep("Down", length(glm.probs))
glm.pred[glm.probs>.5] <-  "Up"
table(glm.pred, Direction)
```

**Percentage of currect predictions: (54+557)/(54+557+48+430) = 56.1%. Weeks the market goes up the logistic regression is right most of the time, 557/(557+48) = 92.1%. Weeks the market goes up the logistic regression is wrong most of the time 54/(430+54) = 11.2%. This is at the level of discussion in class and should be at 15 points. We encouraged them to explore additional approaches (in the lecture). So for additional five points any effort to go beyond plain vanilla of the textbook chapter lab. For example, any of the following: they go for a CARET package implementation; calculate precision, recall, F1; plot ROC and show AUC; and provide a minimal discussion of these results potentially in the context of class imbalance.**

(4) Now fit the logistic regression model using a training data period from 1990 to 2008, with `Lag2` as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010).

```{r}
train <-  (Year < 2009)
Weekly.0910 <-  Weekly[!train,]
glm.fit <-  glm(Direction ~ Lag2, data = Weekly, family = binomial, subset = train)
glm.probs <-  predict(glm.fit, Weekly.0910, type="response")
glm.pred <-  rep("Down", length(glm.probs))
glm.pred[glm.probs>.5] <-  "Up"
Direction.0910 <-  Direction[!train]
table(glm.pred, Direction.0910)
mean(glm.pred == Direction.0910)
```

**Similar to above. If they cover the implementation in the chapter (and in this solution code chunk) that's at least 15. Additional points (up to the total 20) for extra effort in interpreting the results, and thinking through other measures of classification fit.**

(5) Experiment with different combinations of predictors, including possible transformations and interactions, and classification methods. Report the variables, method, and associated confusion matrix that appears to provide the best results on the held out data.

```{r}
# Logistic regression with Lag2:Lag1
glm.fit <-  glm(Direction~Lag2:Lag1, data = Weekly, family = binomial, subset = train)
glm.probs <-  predict(glm.fit, Weekly.0910, type = "response")
glm.pred <-  rep("Down", length(glm.probs))
glm.pred[glm.probs>.5] <-  "Up"
Direction.0910 <-  Direction[!train]
table(glm.pred, Direction.0910)
mean(glm.pred == Direction.0910)
```

```{r}
# LDA with Lag2 interaction with Lag1
lda.fit <-  lda(Direction ~ Lag2:Lag1, data = Weekly, subset = train)
lda.pred <-  predict(lda.fit, Weekly.0910)
mean(lda.pred$class == Direction.0910)
```

```{r}
# QDA with sqrt(abs(Lag2))
qda.fit <-  qda(Direction ~ Lag2 + sqrt(abs(Lag2)), data = Weekly, subset = train)
qda.class <-  predict(qda.fit, Weekly.0910)$class
table(qda.class, Direction.0910)
mean(qda.class == Direction.0910)
```

```{r}
train.X <-  as.matrix(Lag2[train])
test.X <-  as.matrix(Lag2[!train])
train.Direction <-  Direction[train]
set.seed(1)

# KNN k =10
knn.pred <-  knn(train.X, test.X, train.Direction, k = 10)
table(knn.pred, Direction.0910)
mean(knn.pred == Direction.0910)

# KNN k = 100
knn.pred <-  knn(train.X, test.X, train.Direction, k = 100)
table(knn.pred, Direction.0910)
mean(knn.pred == Direction.0910)
```

**Out of these permutations, the original LDA and logistic regression have better performance in terms of test error rate. For the basic answer they should have the  logistic with some twist (like an interaction term), and one of the other classification models we covered (e.g. lda, qda, or knn). They can also have plain vanilla logistic and then at least two more experiments with models like lda or qda. That would cover what we had in the labs and lecture and should be 10 points. For the remaining 10 points it can be more detailed and eleborate coverage of the misclassification analysis (like in solution discussion for Q3 and Q4 above). Or some other models and more eleborate experiments. Essentially we are looking for additional effort over and above what we already covered in the labs for classification day.**

